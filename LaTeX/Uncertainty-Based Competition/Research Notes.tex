\documentclass[10pt]{article}

\usepackage[backend=biber, style=authoryear, sorting=ynt]{biblatex}
\addbibresource{Research.bib}

\title{Research Notes}
\author{Richard Maskell - 1238287}
\begin{document}
\maketitle
\date

\section{Uncertainty-Based Competition Between Prefrontal and Dorsolateral Striatal Systems for Behavioural Control}
	
	\subsection{Document Overview}

		This article by Daw, Niv \& Dayan is based on the idea that ``neural systems, notably prefrontal cortex, the striatum and their dopaminergic afferents, are thought to contribute to the selection of actions'' \parencite{Daw}

		When these systems disagree, the different neurological structures compete with each other, which is modelled here using a ``Bayesian principle of arbitration between them according to uncertainty, so each controller is deployed when it should be most accurate''. \parencite{Daw}


\subsection{The Dorsolateral Striatum}

	It is stated that ``the dorsolateral striatum and its dopaminergic afferents support habitual or reflexive control'' \parencite{Daw}

\subsection{The Prefrontal Cortex}
	
	The prefrontal cortex is said to be ``associated with more reflective or cognitive action planning'' \parencite{Daw} along with additional regions which are excluded for simplicity in this article.

\subsection{Outcome Re-valuation}
	
	Conditioning studies where the subject learns desirable behaviour through rewards and/or punishments can have their reward values unexpectedly changed in order to differentiate between two control systems. ``Outcome re-valuation affects the two styles of control differently and allows investigation of the characteristics of each controller, its neural substrates and the circumstances under which it dominates'' \parencite{Daw}

\subsection{Normative Questions}

	``Why should the brain use multiple action controllers
	How should action choice be determined when they disagree'' \parencite{Daw}


\subsection{Reinforcement Learning}

	``In reinforcement learning, candidate actions are assessed through predictions of their values, defined in terms of the amount of reward they are expected eventually to bring about'' \parencite{Daw}.

	\subsubsection{Deferred Rewards}

		Deferred rewards, such as those dependant on multiple consecutive actions, present complications in predicting the value of actions in reinforcement learning as an initial choice in the sequence may not produce any immediate rewards, only a deferred one. Two different classes of reinforcement learning are used to produce different action's values, which are model-free approaches and model-based approaches. ``We interpret the two controllers as representing opposite extremes in a trade-off between the statistically efficient use of experience and computational tractability'' \parencite{Daw}

	\subsubsection{The Model-Free Approach}

		``[These] underpin existing popular accounts of the activity of dopamine neurons and their (notably dorsolateral) striatal projections'' \parencite{Daw}. One such method is temporal-difference learning which is founded on the principle of `caching' which is ``the association of an action or situation with a scalar summary of its long-run future value.
		A hallmark of this is the ubiquitous transfer of the dopaminergic response from rewards to the stimuli that predict them'' \parencite{Daw} This method is computationally simple but has the disadvantage of the values being separated from the outcomes and thus, do not change when the outcome is re-valued on the fly.


	\subsubsection{The Model-Based Approach}

		``[This approach] involves `model-based' methods, which we identify with the prefrontal cortex system'' \parencite{Daw} The predictions are said to be calculated ``on the fly, by chaining together short-term predictions about the immediate consequences of each action in a sequence'' \parencite{Daw}. Since there is a branching set of situations produced from each action in a state which need to be explored, this is often refered to as `tree search'. 
		This method can be very computationally expensive due to this exploration of deep trees and can also be erroneous as a result of its complexity. 
		However, an advantage of this approach is that the predicted values are able to react flexibly to outcome re-valuation as they are constructed on the fly.

	\subsubsection{Accuracy of Different Reinforcement Learning Approaches}

		The different accuracy ratings of each approach is proposed by \textcite{Daw} to justify ``the plurality of control and [underpin] arbitration'' where the brain relies on the specific control system in the circumstances where it's predictions tend to be most accurate.

		Such accuracy is suggested by \textcite{Daw} to be estimated for the ``purpose of arbitration by tracking the relative uncertainty of the predictions made by each controller.''

		Strict separation between the systems is assumed in order to isolate their hypothesis


\subsection{Results}

	\subsubsection{Post-training Reinforcer Devaluation}

		A typical psychological experiments involving post-training reinforcer devaluation is mentioned by \textcite{Daw} where ``hungry rats are trained to perform a sequence of actions, [...] to obtain a reward such as a food pellet''. The value of the reward is then reduced, by either feeding the rats before hand or by tainting the food pellets to associate an aversion to them for the rats. The test is then repeated to see the effect of the devalued reward on the animal's behaviour. 

		Such experiments can be used to compare different behavioural models such as tree search and caching. The values associated with actions in caching are independant of any outcome information. ``Thus, if an animal acts based on a cached value, it will continue to do so even after the outcome has been devalued'' \parencite{Daw}. Such behaviour is known as `habitual'. ``If, however a behaviour is determined by tree search, its propensity should be sharply reduced following devaluation'' \parencite{Daw}. Such behaviour is known as `goal-directed' and changed when the value of goals is changed.

		Animals have been shown to posess both levels of devaluation sensitivity, with extensive training leading to transfer of control to a caching system from a tree search one. ``Lesions or depletions of dopaminergic input to dorsolateral areas of the striatum evidently block this transfer of control'' \parencite{Daw} leading to behaviour being sensitive to devaluation, even after extensive training.

		The transfer of control is also affected by ``the complexity of action choice and the proximity of the action to reward'' \parencite{Daw}, with complex action choices remaining sensitive to outcome devaluation following extensive training, as well as actions which are closer to the reward remaining more sensitive to devaluation than actions further away.


\subsection{Notes}

\subsection{Conclusions}

\printbibliography

\end{document}